# ğŸ§  Memory Chatbot with Ollama

A privacy-first, locally hosted chatbot app that remembers your conversations and retrieves relevant context using vector memory. Powered by [Streamlit](https://streamlit.io/) and [Ollama](https://ollama.com/) for self-hosted LLM responses.

---

## ğŸš€ Features

### âœ… Local LLM Inference
- Uses **Ollama** to run models like `llama3` entirely on your machine.
- No internet or API keys required for language generation.

### ğŸ§  Long-Term Memory (FAISS)
- Saves interactions (human + assistant) as vector embeddings.
- Recalls relevant past exchanges to improve contextual response.
- View and manage memory from the sidebar.

### ğŸ’¬ Chat Interface
- Clean, real-time chatbot UI powered by Streamlit.
- All conversation history is displayed in context.

### ğŸ—‚ï¸ Persistent Storage
- Stores memory in `data/faiss_index` and metadata in JSON.
- Chat history persists across sessions.

---

## ğŸ“¦ Requirements

- Python 3.8+
- FAISS
- Streamlit
- NumPy
- Requests
- [Ollama](https://ollama.com/) installed and running locally

---

## ğŸ› ï¸ Installation & Run

1. **Clone the repository**
```bash
git clone https://github.com/waelr1985/memory-chatbot.git
cd memory-chatbot
```

2. **Create virtual environment (optional but recommended)**
```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Start Ollama with your model (e.g., llama3)**
```bash
ollama run llama3
```

5. **Run the chatbot app**
```bash
streamlit run simple_memory_chatbot.py
```

---

## ğŸ“ File Structure

```
â”œâ”€â”€ data/                         # Contains FAISS index + metadata
â”œâ”€â”€ simple_memory_chatbot.py     # Main app script
â”œâ”€â”€ .env                         # Optional config (e.g., APP_NAME, USER_IDENTITY)
â”œâ”€â”€ requirements.txt             # Python dependencies
```

---

## âœ¨ Customization

- Change model: edit `CHAT_MODEL` in `.env` (e.g., `llama3`, `mistral`) 
- Modify temperature/max_tokens in `Settings`
- Replace the embedder with real embeddings (e.g., `sentence-transformers`)

---

## ğŸ” Privacy
- This app runs **entirely offline**.
- No data is sent to third-party APIs.
- You control your memory.

---

## ğŸ¤– Future Ideas
- Support multiple users
- Add local embedding models
- Export/import memory
- Use LangChain for prompt chaining

---

## ğŸ“„ License
MIT License. Feel free to fork and enhance!

---

## ğŸ™‹â€â™‚ï¸ Author
Wael Rahhal â€” [LinkedIn](https://www.linkedin.com/in/wael-rahhal-ph-d-06786522/) | [GitHub](https://github.com/waelr1985) 
